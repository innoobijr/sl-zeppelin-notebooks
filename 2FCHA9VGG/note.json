{
  "paragraphs": [
    {
      "text": "/**\n * title: summary_stats\n * author: Innocent Obi Jr\n * description: \n */\n \n//Default configurations (Remove in production)\nsc.hadoopConfiguration.set(\"fs.s3n.awsAccessKeyId\", System.getenv(\"AWS_ID\"))\nsc.hadoopConfiguration.set(\"fs.s3n.awsSecretAccessKey\", System.getenv(\"AWS_SECRET\"))\nval pathToCdrs \u003d \"s3n://sierra-leone-lake/blob/cdrs/africell-raw/20200403_1[4-6].csv\"\nval pathToTowers \u003d \"s3n://sierra-leone-lake/blob/TOWERS/africell/sites_2020-07-03.csv\"\nval pathToOutput \u003d \"s3n://sierra-leone-lake/blob/AGGREGATED_CDRS/africell/one_hop_tower\"",
      "user": "anonymous",
      "dateUpdated": "2020-07-09 09:10:03.098",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "pathToCdrs: String \u003d s3n://sierra-leone-lake/blob/cdrs/africell-raw/20200403_1[4-6].csv\npathToTowers: String \u003d s3n://sierra-leone-lake/blob/TOWERS/africell/sites_2020-07-03.csv\npathToOutput: String \u003d s3n://sierra-leone-lake/blob/AGGREGATED_CDRS/africell/one_hop_tower\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1594143690164_344790806",
      "id": "20200703-215801_239465495",
      "dateCreated": "2020-07-07 17:41:30.164",
      "dateStarted": "2020-07-08 21:47:07.825",
      "dateFinished": "2020-07-08 21:47:08.041",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.sql.functions.{split, lit, window, concat, unix_timestamp, to_timestamp, date_format}\nimport org.apache.spark.sql.types._\n//import org.apache.spark.sql._\nimport org.apache.spark.sql.expressions.Window\nimport spark.implicits._\nimport org.apache.spark.storage.StorageLevel\n\n val calls \u003d spark.read.format(\"csv\")\n    .option(\"header\",\"false\")\n    .option(\"inferSchema\", \"true\")\n    .load(pathToCdrs)\n    .withColumnRenamed(\"_c0\", \"antenna_id\")\n    .withColumnRenamed(\"_c1\", \"time\")\n    .withColumnRenamed(\"_c2\", \"direction\")\n    .withColumnRenamed(\"_c3\", \"source\")\n    .withColumn(\"subscriber\", col(\"source\"))\n    .withColumnRenamed(\"_c4\", \"target\")\n    .withColumnRenamed(\"_c5\", \"duration\")\n    .withColumn(\"site_id\", substring(col(\"antenna_id\"), 0, 4))\n    .withColumn(\"call_datetime\", to_timestamp($\"time\"))\n    .withColumn(\"call_date\", to_date($\"call_datetime\"))\n    .withColumn(\"end_time\",  to_timestamp(unix_timestamp($\"call_datetime\") + ($\"duration\")))\n    \n\nval cells \u003d spark.read.format(\"csv\")\n  .option(\"header\", true)\n  .option(\"inferSchema\", true)\n  .load(pathToTowers)\n  .withColumnRenamed(\"New_Dist\", \"locality\")\n  .withColumnRenamed(\"CI\", \"cell_id\")\n  .withColumnRenamed(\"site_id\", \"_site_id\")\n\n//cells.persist(StorageLevel.MEMORY_AND_DISK)\ncells.createOrReplaceTempView(\"cells\")\n//cells.unpersist()\n\n//calls.persist(StorageLevel.MEMORY_AND_DISK)\ncalls.createOrReplaceTempView(\"calls\")\n//calls.unpersist()\n\nval merged \u003d calls.join(cells, calls.col(\"site_id\") \u003d\u003d\u003d cells.col(\"_site_id\"))\nmerged.persist(StorageLevel.MEMORY_AND_DISK)\nmerged.createOrReplaceTempView(\"merged\")\n    ",
      "user": "anonymous",
      "dateUpdated": "2020-07-08 21:47:12.286",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.functions.{split, lit, window, concat, unix_timestamp, to_timestamp, date_format}\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.expressions.Window\nimport spark.implicits._\nimport org.apache.spark.storage.StorageLevel\ncalls: org.apache.spark.sql.DataFrame \u003d [antenna_id: int, time: timestamp ... 9 more fields]\ncells: org.apache.spark.sql.DataFrame \u003d [_site_id: string, cell_id: int ... 8 more fields]\nmerged: org.apache.spark.sql.DataFrame \u003d [antenna_id: int, time: timestamp ... 19 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1594143690165_2056680510",
      "id": "20200703-234729_1101739060",
      "dateCreated": "2020-07-07 17:41:30.165",
      "dateStarted": "2020-07-08 21:47:12.345",
      "dateFinished": "2020-07-08 21:47:55.220",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// 1. District Subscribers and Sites\nval distinctSubscriberAndCells \u003d merged.select(countDistinct(col(\"subscriber\")), countDistinct(col(\"site_id\")))\n\nval countEventsPerDay \u003d merged.groupBy(col(\"call_date\")).agg(count($\"subscriber\"))\n\nval eventsAndUsersPerSitePerDay \u003d merged.groupBy(\"site_id\", \"call_date\").agg(count($\"subscriber\").alias(\"count\"), countDistinct($\"subscriber\").alias(\"subscribers\")).orderBy($\"count\".desc)\n\nval eventsAndUsersPerLocalityPerDay \u003d merged.groupBy(\"locality\", \"call_date\").agg(count($\"subscriber\").alias(\"events\"), countDistinct($\"subscriber\").alias(\"subscribers\"), countDistinct(col(\"site_id\")).alias(\"sites\"))\n\n//-------\ndistinctSubscriberAndCalls.write.format(\"csv\").option(\"header\", \"true\").save(\"s3n://sierra-leone-lake/blob/AGGREGATED_CDRS/africell/summaries/distinctSubscriberAndCalls.csv\")\ncountEventsPerDay.write.format(\"csv\").option(\"header\", \"true\").save(\"s3n://sierra-leone-lake/blob/AGGREGATED_CDRS/africell/summaries/countEventsPerDay.csv\")\neventsAndUsersPerSitePerDay.write.format(\"csv\").option(\"header\", \"true\").save(\"s3n://sierra-leone-lake/blob/AGGREGATED_CDRS/africell/summaries/eventsAndUsersPerSitePerDay.csv\")\neventsAndUsersPerLocalityPerDay.write.format(\"csv\").option(\"header\", \"true\").save(\"s3n://sierra-leone-lake/blob/AGGREGATED_CDRS/africell/summaries/eventsAndUsersPerLocalityPerDay.csv\")",
      "user": "anonymous",
      "dateUpdated": "2020-07-08 22:17:29.664",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.SparkException: Job aborted.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:677)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:677)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:677)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:286)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:272)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:230)\n  ... 55 elided\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 14 in stage 22.0 failed 1 times, most recent failure: Lost task 14.0 in stage 22.0 (TID 1052, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.ServiceException: S3 Error Message. -- ResponseCode: 404, ResponseStatus: Not Found, XML Error Message: \u003cError\u003e\u003cCode\u003eNoSuchKey\u003c/Code\u003e\u003cMessage\u003eThe specified key does not exist.\u003c/Message\u003e\u003cKey\u003eblob/AGGREGATED_CDRS/africell/summaries/eventsAndUsersPerSitePerDay.csv/_temporary/0/_temporary/attempt_20200708215007_0022_m_000014_1052/part-00014-1b61c094-3700-4e12-8607-1653e11f1de3-c000.csv\u003c/Key\u003e\u003cRequestId\u003e99B8CAAB77010822\u003c/RequestId\u003e\u003cHostId\u003en0PO1QrCJY6i6sO+6h/V6CdqA5V0ZdFPAGMseV6l2LMiIdfKi2OC2TUVaKUrFrXqGwuq+biqJDE\u003d\u003c/HostId\u003e\u003c/Error\u003e\n\tat org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.processException(Jets3tNativeFileSystemStore.java:470)\n\tat org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.handleException(Jets3tNativeFileSystemStore.java:411)\n\tat org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.copy(Jets3tNativeFileSystemStore.java:326)\n\tat sun.reflect.GeneratedMethodAccessor87.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n\tat org.apache.hadoop.fs.s3native.$Proxy15.copy(Unknown Source)\n\tat org.apache.hadoop.fs.s3native.NativeS3FileSystem.rename(NativeS3FileSystem.java:721)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:578)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:549)\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)\n\tat org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n\t... 10 more\nCaused by: org.jets3t.service.ServiceException: S3 Error Message. -- ResponseCode: 404, ResponseStatus: Not Found, XML Error Message: \u003cError\u003e\u003cCode\u003eNoSuchKey\u003c/Code\u003e\u003cMessage\u003eThe specified key does not exist.\u003c/Message\u003e\u003cKey\u003eblob/AGGREGATED_CDRS/africell/summaries/eventsAndUsersPerSitePerDay.csv/_temporary/0/_temporary/attempt_20200708215007_0022_m_000014_1052/part-00014-1b61c094-3700-4e12-8607-1653e11f1de3-c000.csv\u003c/Key\u003e\u003cRequestId\u003e99B8CAAB77010822\u003c/RequestId\u003e\u003cHostId\u003en0PO1QrCJY6i6sO+6h/V6CdqA5V0ZdFPAGMseV6l2LMiIdfKi2OC2TUVaKUrFrXqGwuq+biqJDE\u003d\u003c/HostId\u003e\u003c/Error\u003e\n\tat org.jets3t.service.impl.rest.httpclient.RestStorageService.performRequest(RestStorageService.java:427)\n\tat org.jets3t.service.impl.rest.httpclient.RestStorageService.performRequest(RestStorageService.java:281)\n\tat org.jets3t.service.impl.rest.httpclient.RestStorageService.performRestPut(RestStorageService.java:1043)\n\tat org.jets3t.service.impl.rest.httpclient.RestStorageService.copyObjectImpl(RestStorageService.java:2029)\n\tat org.jets3t.service.StorageService.copyObject(StorageService.java:871)\n\tat org.jets3t.service.StorageService.copyObject(StorageService.java:916)\n\tat org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.copy(Jets3tNativeFileSystemStore.java:323)\n\t... 30 more\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n  ... 76 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows.\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\nCaused by: org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.ServiceException: S3 Error Message. -- ResponseCode: 404, ResponseStatus: Not Found, XML Error Message: \u003cError\u003e\u003cCode\u003eNoSuchKey\u003c/Code\u003e\u003cMessage\u003eThe specified key does not exist.\u003c/Message\u003e\u003cKey\u003eblob/AGGREGATED_CDRS/africell/summaries/eventsAndUsersPerSitePerDay.csv/_temporary/0/_temporary/attempt_20200708215007_0022_m_000014_1052/part-00014-1b61c094-3700-4e12-8607-1653e11f1de3-c000.csv\u003c/Key\u003e\u003cRequestId\u003e99B8CAAB77010822\u003c/RequestId\u003e\u003cHostId\u003en0PO1QrCJY6i6sO+6h/V6CdqA5V0ZdFPAGMseV6l2LMiIdfKi2OC2TUVaKUrFrXqGwuq+biqJDE\u003d\u003c/HostId\u003e\u003c/Error\u003e\n  at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.processException(Jets3tNativeFileSystemStore.java:470)\n  at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.handleException(Jets3tNativeFileSystemStore.java:411)\n  at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.copy(Jets3tNativeFileSystemStore.java:326)\n  at sun.reflect.GeneratedMethodAccessor87.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)\n  at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)\n  at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)\n  at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)\n  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)\n  at org.apache.hadoop.fs.s3native.$Proxy15.copy(Unknown Source)\n  at org.apache.hadoop.fs.s3native.NativeS3FileSystem.rename(NativeS3FileSystem.java:721)\n  at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:578)\n  at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitTask(FileOutputCommitter.java:549)\n  at org.apache.spark.mapred.SparkHadoopMapRedUtil$.performCommit$1(SparkHadoopMapRedUtil.scala:50)\n  at org.apache.spark.mapred.SparkHadoopMapRedUtil$.commitTask(SparkHadoopMapRedUtil.scala:77)\n  at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitTask(HadoopMapReduceCommitProtocol.scala:225)\n  at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.commit(FileFormatDataWriter.scala:78)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:247)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)\n  at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)\n  ... 10 more\nCaused by: org.jets3t.service.ServiceException: S3 Error Message.\n  at org.jets3t.service.impl.rest.httpclient.RestStorageService.performRequest(RestStorageService.java:427)\n  at org.jets3t.service.impl.rest.httpclient.RestStorageService.performRequest(RestStorageService.java:281)\n  at org.jets3t.service.impl.rest.httpclient.RestStorageService.performRestPut(RestStorageService.java:1043)\n  at org.jets3t.service.impl.rest.httpclient.RestStorageService.copyObjectImpl(RestStorageService.java:2029)\n  at org.jets3t.service.StorageService.copyObject(StorageService.java:871)\n  at org.jets3t.service.StorageService.copyObject(StorageService.java:916)\n  at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.copy(Jets3tNativeFileSystemStore.java:323)\n  ... 30 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1594143690165_787814584",
      "id": "20200703-235005_1368662916",
      "dateCreated": "2020-07-07 17:41:30.165",
      "dateStarted": "2020-07-08 21:47:16.225",
      "dateFinished": "2020-07-08 21:50:28.037",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// Towers in CDR that are not in the tower dataset\nval call_sites \u003d calls.select($\"site_id\").distinct()\nval tower_sites \u003d cells.select($\"_site_id\").distinct()\nval call_antennas \u003d calls.select($\"antenna_id\").distinct()\nval tower_antennas \u003d cells.select($\"cell_id\").distinct()\n\nval missing_sites \u003d call_sites.join(tower_sites, call_sites.col(\"site_id\") \u003d\u003d\u003d tower_sites.col(\"_site_id\"), \"leftanti\")\nmissing_sites.write.format(\"csv\").option(\"header\", \"true\").save(\"s3n://sierra-leone-lake/blob/AGGREGATED_CDRS/africell/summaries/missing_sites\")\n\nval missing_cells \u003d call_sites.join(tower_sites, call_sites.col(\"antenna_id\") \u003d\u003d\u003d tower_sites.col(\"cell_id\"), \"leftanti\")\nmissing_cells.write.format(\"csv\").option(\"header\", \"true\").save(\"s3n://sierra-leone-lake/blob/AGGREGATED_CDRS/africell/summaries/missing_cells\")",
      "user": "anonymous",
      "dateUpdated": "2020-07-09 09:11:41.538",
      "config": {
        "colWidth": 12.0,
        "fontSize": 9.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "call_sites: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [site_id: string]\ntower_sites: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [_site_id: string]\ncall_antennas: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [antenna_id: int]\ntower_antennas: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [cell_id: int]\nres9: Long \u003d 104\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1594243119271_-722267792",
      "id": "20200708-211839_753603448",
      "dateCreated": "2020-07-08 21:18:39.271",
      "dateStarted": "2020-07-08 21:29:58.975",
      "dateFinished": "2020-07-08 21:30:30.771",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "MIT/summary_stats",
  "id": "2FCHA9VGG",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {
    "spark:shared_process": []
  },
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}